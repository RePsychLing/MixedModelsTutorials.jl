{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Algebra - solving least squares problems\n### Douglas Bates\n\n# Linear algebra and statistical models\n\n`Julia` provides one of the best, if not _the best_, environments for numerical linear algebra.\n\nThe `Base` package provides basic array (vector, matrix, etc.) construction and manipulation; `*`, `/`, `\\`, `'`.  The `LinearAlgebra` package provides many definitions of matrix types (`Diagonal`, `UpperTriangular`, ...) and factorizations.  The `SparseArrays` packages provides types and methods for sparse matrices."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = rand(4,3)   # simulate a matrix with elements selected at random from (0,1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "B = rand(3, 4)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A'   # \"lazy\" transpose"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A*B"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving least squares problems\n\nA least squares solution is one of the building blocks for statistical models.\n\nIf `X` is an $n\\times p$ model matrix and `y` is an $n$-dimensional vector of observed responses, a _linear model_ is of the form\n$$\n\\mathcal{Y}\\sim\\mathcal{N}(\\mathbf{X}\\beta, \\sigma^2\\mathbf{I})\n$$\n\nThat is, the _mean response_ is modeled as a _linear predictor_ , $\\mathbf{X}\\beta$, depending on the _parameter vector_, $\\beta$ (also called _coefficients_) with the covariance matrix, $\\sigma^2\\mathbf{I}$.  In general the probability density for a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) with mean $\\mu$ and covariance matrix $\\Sigma$ is\n$$\n  f(\\mathbf{y}|\\mu,\\Sigma) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}\n     \\exp\\left(-\\frac{(\\mathbf{y}-\\mu)'\\Sigma^{-1}(\\mathbf{y}-\\mu)}{2}\\right)\n$$\nwhere $|\\Sigma|$ denotes the determinant of $\\Sigma$.\n\nWhen the covariance matrix is of the form $\\sigma^2\\mathbf{I}$ the distribution is called a _spherical_ normal\ndistribution because the contours of constant density are $n$-dimensional spheres centered at $\\mu$.\nIn these cases the probability density can be simplified to\n$$\n\\begin{aligned}\n   f(\\mathbf{y}|\\beta,\\sigma)&= \\frac{1}{(2\\pi\\sigma^2)^{n/2}} \\exp\\left(-\\frac{(\\mathbf{y}-\\mathbf{X}\\beta)'(\\mathbf{y}-\\mathbf{X}\\beta)}{2\\sigma^2}\\right)\\newline\n   &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}} \\exp\\left(-\\frac{\\|\\mathbf{y}-\\mathbf{X}\\beta)\\|^2}{2\\sigma^2}\\right) .\n\\end{aligned}\n$$\n\nThe _likelihood_ of the parameters, $\\beta$ and $\\sigma$, given the data, $\\mathbf{y}$ (and, implicitly, $\\mathbf{X}$), is the same expression as the density but with the roles of the parameters and the observations reversed\n$$\nL(\\beta,\\sigma|\\mathbf{y})=\\frac{1}{(2\\pi\\sigma^2)^{n/2}} \\exp\\left(-\\frac{\\|\\mathbf{y}-\\mathbf{X}\\beta)\\|^2}{2\\sigma^2}\\right) .\n$$\n\nThe _maximum likelihood estimates_ of the parameters are the values that maximize the likelihood given the data.  It is convenient to maximize the logarithm of the likelihood, called the _log-likelihood_, instead of the likelihood.\n$$\n\\ell(\\beta,\\sigma|\\mathbf{y})=\\log L(\\beta,\\sigma|\\mathbf{y})=\n-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{\\|\\mathbf{y}-\\mathbf{X}\\beta)\\|^2}{2\\sigma^2}\n$$\n\n(Because the logarithm function is monotone increasing, the values of $\\beta$ and $\\sigma$ that maximize the log-likelihood also maximize the likelihood.)\n\nFor any value of $\\sigma$ the value of $\\beta$ that maximizes the log-likelihood is the value that minimizes the sum of squared residuals,\n$$\n\\widehat{\\beta}=\\arg\\min_\\beta \\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2\n$$\n\n## A simple linear regression model\n\nData from a calibration experiment on the optical density versus the concentration of Formaldehyde are available as the `Formaldehyde` data in `R`.  We use the `RCall` package to retrieve these data from `R`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using LinearAlgebra, RCall, StatsModels, Tables"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Formaldehyde = rcopy(R\"Formaldehyde\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "R\"\"\"\nlibrary(ggplot2)\nqplot(x=carb, y=optden, data=Formaldehyde, geom=\"point\")\n\"\"\""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a _simple linear regression_ the model matrix, $\\mathbf{X}$, consists of a column of 1's and a column of the covariate values; `carb`, in this case."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "X = hcat(ones(size(Formaldehyde, 1)), Formaldehyde.carb)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "y = Formaldehyde.optden"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "β = X\\y     # least squares estimate"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "r = y - X*β   #residual"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the conditions for $\\hat{\\beta}$ being the least squares estimate is that the residuals must be orthogonal to the columns of $\\mathbf{X}$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "X'r   # not exactly zero but very small entries"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the model matrix from a formula\n\nCreating model matrices from a data table can be a tedious and error-prone operation.  In addition, _statistical inference_ regarding a linear model often considers groups of columns generated by model _terms_.  The `GLM` package provides methods to fit and analyze linear models and generalized linear models (described later) using a _formula/data_ specification similar to that in _R_."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using GLM"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "m1 = fit(LinearModel, @formula(optden ~ 1 + carb), Formaldehyde)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation of the formula is performed by the `StatsModels` package in stages."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f1 = @formula(optden ~ 1 + carb)\ny, X = modelcols(apply_schema(f1, schema(Formaldehyde)), Formaldehyde)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix decompositions for least squares\n\nAccording to the formulas given in text books, the least squares estimates are calculated as\n$$\n\\widehat{\\mathbf{\\beta}}=\\mathbf{X^\\prime X}^{-1}\\mathbf{X^\\prime y}\n$$\n\nIn practice, this formula is not the way the estimates are calculated, because it is wasteful to evaluate the inverse of a matrix if you just want to solve a system of equations.\n\nRecall that the least squares estimate satisfies the condition that the residual is orthogonal to the columns of $\\mathbf{X}$.\n$$\n\\mathbf{X^\\prime (y - X\\widehat{\\beta})} = \\mathbf{0}\n$$\nwhich can be re-written as\n$$\n\\mathbf{X^\\prime X}\\widehat{\\mathbf{\\beta}}=\\mathbf{X^\\prime y}\n$$\nThese are called the _normal equations_ - \"normal\" in the sense of orthogonal, not in the sense of the normal distribution.\n\nThe matrix $\\mathbf{X^\\prime X}$ is symmetric and _positive definite_.  The latter condition means that\n$$\n\\mathbf{v^\\prime(X^\\prime X)v}=\\mathbf{(Xv)^\\prime Xv} = \\|\\mathbf{Xv}\\|^2 > 0\\quad\\forall \\mathbf{v}\\ne\\mathbf{0}\n$$\nif $\\mathbf{X}$ has full column rank.\n\nWe will assume that the model matrices $\\mathbf{X}$ we will use do have full rank.  It is possible to handle rank-deficient model matrices but we will not cover that here.\n\nA positive-definite matrix has a \"square root\" in the sense that there is a $p\\times p$ matrix $\\mathbf{A}$ such that\n$$\\mathbf{A^\\prime A}=\\mathbf{X^\\prime X} .$$\nIn fact, when $p>1$ there are several.  A specific choice of $\\mathbf{A}$ is an upper triangular matrix with positive elements on the diagonal, usually written $\\mathbf{R}$ and called the upper Cholesky factor of $\\mathbf{X^\\prime X}$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "xpx = X'X"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ch = cholesky(xpx)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ch.U'ch.U"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ch.U'ch.U ≈ xpx"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the Cholesky factor is triangular, it is possible to solve systems of equations of the form\n$$\\mathbf{R^\\prime R}\\widehat{\\mathbf{\\beta}}=\\mathbf{X^\\prime y}$$\nin place in two stages.  First solve for $\\mathbf{v}$ in\n$$\\mathbf{R^\\prime v}=\\mathbf{X^\\prime y}$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "v = ldiv!(ch.U', X'y)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "then solve for $\\widehat{\\mathbf{\\beta}}$ in\n$$\n\\mathbf{R}\\widehat{\\mathbf{\\beta}}=\\mathbf{v}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "βc = ldiv!(ch.U, copy(v))     # solution from the Cholesky factorization"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "βc ≈ β"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "These steps are combined in one of the many `LinearAlgebra` methods for solutions of equations."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ldiv!(ch, X'y)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sum of squared residuals as a quadratic form\n\nAnother way of approaching the least squares problem is to write the sum of squared residuals as what is called a _quadratic form_.\n$$\n\\begin{aligned}\nr^2(\\mathbf{\\beta}) & = \\|\\mathbf{y} - \\mathbf{X\\beta}\\|^2\\\\\n&=\\left\\|\\begin{bmatrix}\\mathbf{X}&\\mathbf{y}\\end{bmatrix}\\begin{bmatrix}\\mathbf{-\\beta}\\\\ 1\\end{bmatrix}\\right\\|^2\\\\\n&=\\begin{bmatrix}\\mathbf{-\\beta}&1\\end{bmatrix}\\begin{bmatrix}\\mathbf{X^\\prime X} & \\mathbf{X^\\prime y}\\\\\n  \\mathbf{y^\\prime X}&\\mathbf{y^\\prime y}\\end{bmatrix}\n  \\begin{bmatrix}\\mathbf{-\\beta}\\\\ 1\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\\mathbf{-\\beta}&1\\end{bmatrix}\n  \\begin{bmatrix}\n    \\mathbf{R_{XX}}^\\prime&\\mathbf{0}\\\\\n    \\mathbf{r_{Xy}}^\\prime&r_{\\mathbf{yy}}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    \\mathbf{R_{XX}}&\\mathbf{r_{Xy}}\\\\\n    \\mathbf{0}&r_{\\mathbf{yy}}\n  \\end{bmatrix}\n  \\begin{bmatrix}\\mathbf{-\\beta}\\\\ 1\\end{bmatrix}\\\\\n&= \\left\\|  \\begin{bmatrix}\n    \\mathbf{R_{XX}}&\\mathbf{r_{Xy}}\\\\\n    \\mathbf{0}&r_{\\mathbf{yy}}\n  \\end{bmatrix}\n  \\begin{bmatrix}\\mathbf{-\\beta}\\\\ 1\\end{bmatrix}\\right\\|^2\\\\\n&= \\|\\mathbf{r_{Xy}}-\\mathbf{R_{XX}\\beta}\\|^2 + r_{\\mathbf{yy}}^2\n\\end{aligned}\n$$\nwhere $\\begin{bmatrix}\\mathbf{R_{XX}}&\\mathbf{r_{Xy}}\\\\ \\mathbf{0}&r_{\\mathbf{yy}}\\end{bmatrix}$ is the upper Cholesky factor of the augmented matrix\n$\\begin{bmatrix}\\mathbf{X^\\prime X} & \\mathbf{X^\\prime y}\\\\ \\mathbf{y^\\prime X}&\\mathbf{y^\\prime y}\\end{bmatrix}$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Xy = hcat(X, y)              # augmented model matrix"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "cha = cholesky(Xy'Xy)        # augmented Cholesky factor"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that $\\mathbf{R_{XX}}$ is just the Cholesky factor of $\\mathbf{X^\\prime X}$ which was previously calculated and the vector $\\mathbf{r_{Xy}}$ is the solution to $\\mathbf{R^\\prime v}=\\mathbf{X^\\prime y}$.  The minimum sum of squares is $r^{\\mathbf{yy}}$ which is attained when $\\mathbf{\\beta}$ is the solution to\n$$\\mathbf{R_{XX}}\\widehat{\\beta}=\\mathbf{r_{Xy}}$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "RXX = UpperTriangular(view(cha.U, 1:2, 1:2))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "RXX ≈ ch.U"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "rXy = cha.U[1:2, end]     # creates a copy (\"view\" doesn't copy)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "rXy ≈ v"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "βac = ldiv!(RXX, copy(rXy))     # least squares solution from the augmented Cholesky"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "βac ≈ β"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "abs2(cha.U[end,end]) ≈ sum(abs2, y - X*β)   # check on residual sum of squares"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "One reason for writing the least squares solution in this way is that we will use a similar decomposition for linear mixed models later.  Another is that when dealing with very large data sets we may wish to parallelize the calculation over many cores or over many processors.  The natural way to parallelize the calculation is in blocks of rows and the augmented Cholesky can be formed row by row using a `lowrankupdate`.\n\n### A row-wise approach to least squares\n\nTo show this we create a row-oriented table from our `Formaldehyde` data set, which is in a column-oriented format."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Formrows = Tables.rowtable(Formaldehyde)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "then initialize a Cholesky factor and zero out its contents"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "chr = cholesky(zeros(3, 3) + I)  # initialize"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fill!(chr.factors, 0);   # zero out the contents\nchr"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update by rows"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fill!(chr.factors, 0)\nfor r in Formrows\n    lowrankupdate!(chr, [1.0, r.carb, r.optden])\nend\nchr"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the same augmented Cholesky factor as before but obtained in a different way.  For generalized linear mixed models and for nonlinear mixed models it can be an advantage to work row-wise when performing some of the least squares calculations.\n\n### Other decompositions for least squares solutions\n\nThere are other ways of solving a least squares problem, such as using an _orthogonal-triangular_ decomposition, also called a _QR_ decomposition, or a singular value decomposition.  The bottom line is that we decompose $\\mathbf{X}$ or $\\mathbf{X^\\prime X}$ into some convenient product of orthogonal or triangular or diagonal matrices and work with those.  Just to relate these ideas, the _QR_ decomposition of $\\mathbf{X}$ is"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "qrX = qr(X)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the `R` factor is the upper Cholesky factor of $\\mathbf{X^\\prime X}$ with the first row multiplied by -1 so its transposed product with itself is $\\mathbf{X^\\prime X}$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "qrX.R'qrX.R"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is,"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "qrX.R'qrX.R ≈ xpx"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original solution for $\\widehat{\\mathbf{\\beta}}$ from the expression `X\\y` is actually performed by taking a QR decomposition of `X`."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.2.0"
    },
    "kernelspec": {
      "name": "julia-1.2",
      "display_name": "Julia 1.2.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
